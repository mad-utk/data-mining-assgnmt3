# -*- coding: utf-8 -*-
"""CRISP-DM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VtrHgrYTBvCL1nLhQM7pHEiEdEJ-AWVq

# **CRISP-DM methodology to Process Bank Marketing Data**

 Lets perform Data Loading and Understanding step
"""

# mount google drive
from google.colab import drive
drive.mount("/content/gdrive")
bank_path = '/content/gdrive/MyDrive/Sem-I/CMPE-255 Data Mining/Assignments3/CRISP-DM/bank.csv'

# read data
import pandas as pd
bank_data = pd.read_csv(bank_path)
# check data
bank_data.head()

"""It appears that the data is separated by semicolons (;) rather than the typical comma delimiter. We'll need to adjust our data reading method to handle this format properly.

Let's reload the data with the correct delimiter and then take a look at its structure.
"""

# Reload the dataset with the correct delimiter
bank_data = pd.read_csv(bank_path, delimiter=';')

# Display the first few rows of the dataset
bank_data.head()

"""let's gather some general statistics about the dataset to understand its distributions, missing values, and other attributes."""

# General statistics about the dataset
summary = bank_data.describe(include='all')

# Checking for missing values
missing_values = bank_data.isnull().sum()

summary, missing_values

"""Let's start by encoding the categorical variables. We'll use one-hot encoding for this purpose."""

# One-hot encoding the categorical variables
bank_data_encoded = pd.get_dummies(bank_data, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'], drop_first=True)

# Display the first few rows of the encoded dataset
bank_data_encoded.head()

"""Next, let's standardize the numerical variables. Standardizing the data can help certain algorithms (like those that use distance measures) converge faster and perform better. We'll use the StandardScaler from sklearn to achieve this.

The numerical features we'll standardize are: 'age', 'balance', 'day', 'duration', 'campaign', 'pdays', and 'previous'.
"""

from sklearn.preprocessing import StandardScaler

# List of numerical features to be standardized
numerical_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

# Initialize the StandardScaler
scaler = StandardScaler()

# Standardize the numerical features
bank_data_encoded[numerical_features] = scaler.fit_transform(bank_data_encoded[numerical_features])

# Display the first few rows of the standardized dataset
bank_data_encoded.head()

"""The numerical features have been standardized, and their values are now centered around zero with a standard deviation of one.

Next, let's check for outliers in the numerical features. Outliers can potentially distort the predictions and affect the accuracy of the models. We'll visualize the distribution of these features using box plots to identify potential outliers.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Visualizing the distribution of numerical features using box plots
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(y=bank_data_encoded[col])
    plt.title(col)
    plt.ylabel('')

plt.tight_layout()
plt.show()

"""Let's begin by capping the outliers for the "balance" and "campaign" features using the methods mentioned above."""

# Handling outliers for 'balance' using the IQR method
Q1_balance = bank_data_encoded['balance'].quantile(0.25)
Q3_balance = bank_data_encoded['balance'].quantile(0.75)
IQR_balance = Q3_balance - Q1_balance

# Capping values
bank_data_encoded['balance'] = bank_data_encoded['balance'].clip(lower=Q1_balance - 1.5 * IQR_balance, upper=Q3_balance + 1.5 * IQR_balance)

# Handling outliers for 'campaign' by capping at the 99th percentile
cap_campaign = bank_data_encoded['campaign'].quantile(0.99)
bank_data_encoded['campaign'] = bank_data_encoded['campaign'].clip(upper=cap_campaign)

# Handling outliers for 'previous' by capping at the 99th percentile
cap_previous = bank_data_encoded['previous'].quantile(0.99)
bank_data_encoded['previous'] = bank_data_encoded['previous'].clip(upper=cap_previous)

# Re-visualizing the box plots after outlier treatment
plt.figure(figsize=(15, 10))
for i, col in enumerate(['balance', 'campaign', 'previous'], 1):
    plt.subplot(3, 3, i)
    sns.boxplot(y=bank_data_encoded[col])
    plt.title(col)
    plt.ylabel('')

plt.tight_layout()
plt.show()

"""Lets, proceed using traditional machine learning libraries such as Scikit-learn.
Here's the plan:

- Split the data into training and test sets.
- Train a few commonly used classifiers and compare their performance.
- Fine-tune the top-performing model.
- Evaluate the model on various metrics.
"""

from sklearn.model_selection import train_test_split

# Splitting the data into training and test sets (80% train, 20% test)
X = bank_data_encoded.drop('y', axis=1)
y = bank_data_encoded['y']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train.shape, X_test.shape

"""Next, we'll train a few commonly used classifiers and compare their performance:

- Logistic Regression
- Random Forest
- Gradient Boosting
- Support Vector Machine
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Initializing the models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Support Vector Machine": SVC(random_state=42)
}

# Training and evaluating the models
results = {}
for name, model in models.items():
    # Training the model
    model.fit(X_train, y_train)

    # Predicting on the test set
    y_pred = model.predict(X_test)

    # Evaluating the model
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=['No', 'Yes'])

    results[name] = (accuracy, report)

results["Logistic Regression"]

"""The Logistic Regression model achieved an accuracy of approximately 89.17% on the test set.

Here's the detailed performance:

Precision (No): 0.91

Recall (No): 0.97

F1-score (No): 0.94

Precision (Yes): 0.55

Recall (Yes): 0.30

F1-score (Yes): 0.39

The model performs well in identifying clients who will not subscribe to a term deposit (labeled "No"). However, its performance in identifying clients who will subscribe (labeled "Yes") is relatively lower.
"""

# Displaying the results for the other models
results["Random Forest"], results["Gradient Boosting"], results["Support Vector Machine"]

"""Random Forest:
- Accuracy: ~89.06%
- Precision (No): 0.91, Recall (No): 0.97, F1-score (No): 0.94
- Precision (Yes): 0.55, Recall (Yes): 0.25, F1-score (Yes): 0.34

Gradient Boosting:
- Accuracy: ~88.73%
- Precision (No): 0.92, Recall (No): 0.96, F1-score (No): 0.94
- Precision (Yes): 0.51, Recall (Yes): 0.37, F1-score (Yes): 0.43

Support Vector Machine:
- Accuracy: ~89.28%
- Precision (No): 0.91, Recall (No): 0.98, F1-score (No): 0.94
- Precision (Yes): 0.59, Recall (Yes): 0.21, F1-score (Yes): 0.31

All models have a similar performance, with the Support Vector Machine having a slight edge in accuracy. However, all models demonstrate a challenge in accurately predicting clients who will subscribe (labeled "Yes").

To further improve performance, we can fine-tune the parameters of the top-performing model. The Support Vector Machine (SVM) seems to be the best candidate.

Fine-tuning involves adjusting the hyperparameters of a model to optimize its performance. We'll use the GridSearchCV method from Scikit-learn to search for the best hyperparameters for our SVM.

For the SVM, some crucial hyperparameters to consider are:

- C: Regularization parameter. The strength of the regularization is inversely proportional to C.
- kernel: Specifies the kernel type to be used in the algorithm. Examples include 'linear', 'poly', 'rbf', etc.
- gamma: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.
We'll perform a grid search over a subset of these hyperparameters to find the optimal values. Let's start the fine-tuning process.
"""

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their possible values
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# Initialize the GridSearchCV object
grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')

# Perform the grid search
grid_search.fit(X_train, y_train)

# Get the best parameters and the best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_params, best_score

"""The best hyperparameters for the Support Vector Machine (SVM) based on our grid search are:

- C: 10
- kernel: 'rbf'
- gamma: 'auto'

With these hyperparameters, the SVM achieved an average accuracy of approximately 90.07% on the training set using 5-fold cross-validation.
"""

# Using the best estimator from the grid search to predict on the test set
best_svm = grid_search.best_estimator_
y_pred_best_svm = best_svm.predict(X_test)

# Evaluate the performance of the best SVM
accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)
report_best_svm = classification_report(y_test, y_pred_best_svm, target_names=['No', 'Yes'])

accuracy_best_svm, report_best_svm

"""The fine-tuned Support Vector Machine (SVM) achieved an accuracy of approximately 88.95% on the test set.

Here's the detailed performance:

- Precision (No): 0.91
- Recall (No): 0.97
- F1-score (No): 0.94
- Precision (Yes): 0.54
- Recall (Yes): 0.29
- F1-score (Yes): 0.37

The SVM continues to perform well in identifying clients who will not subscribe to a term deposit (labeled "No"). The performance for predicting clients who will subscribe (labeled "Yes") has shown a slight improvement after fine-tuning, but there's still room for improvement.

# **Evaluation**
From the modeling phase, the SVM emerged as the best performer among the models we considered. While the model exhibits strong performance in predicting clients who won't subscribe, it has challenges in accurately predicting those who will.

In practical terms, this means the bank might miss out on some potential subscribers but will be confident about those predicted not to subscribe. Depending on the bank's goals (e.g., maximizing the number of subscribers or minimizing false positives), this model might be suitable.
"""